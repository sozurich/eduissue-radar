
import streamlit as st
import pandas as pd
import re
from collections import Counter
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup

def parse_kakao_text(file):
    text = file.read().decode('utf-8')
    lines = text.splitlines()
    parsed = []
    current_date = None
    date_pattern = r'-{10,}\s*(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼.*?)\s*-{10,}'
    msg_pattern = r'\[(.*?)\] \[(ì˜¤ì „|ì˜¤í›„) (\d{1,2}:\d{2})\] (.+)'
    for line in lines:
        date_match = re.match(date_pattern, line)
        if date_match:
            current_date = date_match.group(1)
            continue
        msg_match = re.match(msg_pattern, line)
        if msg_match and current_date:
            user, ampm, time, msg = msg_match.groups()
            hour, minute = map(int, time.split(':'))
            if ampm == 'ì˜¤í›„' and hour != 12:
                hour += 12
            timestamp = f"{hour:02}:{minute:02}"
            parsed.append({"ë‚ ì§œ": current_date, "ì‚¬ìš©ì": user, "ì‹œê°„": timestamp, "ë©”ì‹œì§€": msg})
    return pd.DataFrame(parsed)

issue_keywords = ["ë°°ì†¡", "ì§€ì—°", "ëˆ„ë½", "ë¶ˆëŸ‰", "ë¶€ì¡±", "ì •ì‚°", "ë°˜í’ˆ", "ì¶”ê°€", "ì˜¤ë¥˜"]
def extract_issues(df):
    msgs = df[df['ë©”ì‹œì§€'].str.contains('|'.join(issue_keywords))]
    all_words = ' '.join(msgs['ë©”ì‹œì§€'].tolist())
    nouns = re.findall(r'[\uAC00-\uD7A3]+', all_words)
    count = Counter(nouns)
    return msgs, count.most_common(10)

def crawl_naver_news(query):
    headers = {"User-Agent": "Mozilla/5.0"}
    url = f"https://search.naver.com/search.naver?where=news&query={query}"
    res = requests.get(url, headers=headers)
    if res.status_code != 200:
        return []
    soup = BeautifulSoup(res.text, 'html.parser')
    items = soup.select('.list_news .news_area')
    results = []
    seen = set()
    for item in items:
        title_tag = item.select_one('.news_tit')
        if not title_tag:
            continue
        title = title_tag.text.strip()
        if title in seen:
            continue
        seen.add(title)
        link = title_tag['href']
        press = item.select_one('.info_group span.press').text if item.select_one('.info_group span.press') else ''
        date_text = item.select_one('.info_group .date').text if item.select_one('.info_group .date') else ''
        try:
            pub_date = datetime.strptime(date_text, "%Y.%m.%d. %H:%M")
            display_date = pub_date.strftime('%Y-%m-%d')
        except:
            pub_date = datetime.now()
            display_date = date_text or 'ë‚ ì§œ ì •ë³´ ì—†ìŒ'
        results.append({"ì œëª©": title, "ë§í¬": link, "ì–¸ë¡ ì‚¬": press, "ë‚ ì§œ": pub_date, "í‘œì‹œë‚ ì§œ": display_date})
    results.sort(key=lambda x: x['ë‚ ì§œ'], reverse=True)
    return results

def render_articles(articles):
    if not articles:
        st.markdown("ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for article in articles[:5]:
            with st.container():
                st.markdown(f"**{article['ì œëª©']}** <{article['ì–¸ë¡ ì‚¬']}> ({article['í‘œì‹œë‚ ì§œ']})")
                st.link_button("ğŸ”— ë‰´ìŠ¤ ë³´ëŸ¬ê°€ê¸°", url=article["ë§í¬"])

st.title("ğŸ“š EduIssue Radar")
st.markdown("êµê³¼ì„œ ë¯¼ì› ë©”ì‹œì§€ + ìµœì‹  ë„¤ì´ë²„ ë‰´ìŠ¤ (ìµœê·¼ 7ì¼) ë¶„ì„ê¸°")

uploaded = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ ì±„íŒ… .txt íŒŒì¼ ì—…ë¡œë“œ", type="txt")
if uploaded:
    df = parse_kakao_text(uploaded)
    df['ë‚ ì§œ'] = pd.to_datetime(
        df['ë‚ ì§œ'].str.extract(r'(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼)')[0],
        format='%Yë…„ %mì›” %dì¼',
        errors='coerce'
    ).fillna(pd.Timestamp.today())
    min_d, max_d = df['ë‚ ì§œ'].min().date(), df['ë‚ ì§œ'].max().date()
    st.markdown(f"**ë¶„ì„ ê°€ëŠ¥í•œ ë‚ ì§œ:** {min_d} ~ {max_d}")
    start_d, end_d = st.date_input("ë¶„ì„ ê¸°ê°„ ì„ íƒ", [min_d, max_d])
    df_sel = df[(df['ë‚ ì§œ'] >= pd.to_datetime(start_d)) & (df['ë‚ ì§œ'] <= pd.to_datetime(end_d))]

    tab1, tab2 = st.tabs(["ğŸ“Š ë¯¼ì› ë¶„ì„", "ğŸ“° ìµœì‹  ë„¤ì´ë²„ ë‰´ìŠ¤"])

    with tab1:
        st.success(f"{start_d} ~ {end_d} ë©”ì‹œì§€ {len(df_sel)}ê±´ ë¶„ì„")
        issue_df, top_issues = extract_issues(df_sel)
        st.subheader("ğŸš¨ ë¯¼ì› ë©”ì‹œì§€")
        st.write(issue_df[['ë‚ ì§œ', 'ì‹œê°„', 'ì‚¬ìš©ì', 'ë©”ì‹œì§€']])
        st.markdown("**ë¯¼ì› í‚¤ì›Œë“œ TOP10**")
        for i in range(0, len(top_issues), 3):
            cols = st.columns(3)
            for j, (kw, cnt) in enumerate(top_issues[i:i+3]):
                cols[j].markdown(f"- **{kw}** ({cnt}íšŒ)")

    with tab2:
        st.subheader("ğŸ“° ì—°ê´€ ë‰´ìŠ¤ (ìµœê·¼ 7ì¼)")
        threshold = datetime.now() - timedelta(days=7)
        _, top_issues = extract_issues(df_sel)
        extra_topics = [kw for kw, _ in top_issues[:3]]
        for word in extra_topics:
            with st.expander(f"ğŸ” {word} ê´€ë ¨ ë‰´ìŠ¤"):
                arts = [a for a in crawl_naver_news(word) if a['ë‚ ì§œ'] >= threshold]
                render_articles(arts)

        st.subheader("ğŸ“š ì£¼ì œë³„ ìµœì‹  ë‰´ìŠ¤ (ìµœê·¼ 7ì¼)")
        topics = ["êµê³¼ì„œ", "AI ë””ì§€í„¸êµê³¼ì„œ", "ë¹„ìƒêµìœ¡", "ì²œì¬êµìœ¡", "ì²œì¬êµê³¼ì„œ", "ë¯¸ë˜ì—”", "ì•„ì´ìŠ¤í¬ë¦¼ë¯¸ë””ì–´", "ë™ì•„ì¶œíŒ", "ì§€í•™ì‚¬"]
        for topic in topics:
            with st.expander(f"ğŸ“˜ {topic} ê´€ë ¨ ë‰´ìŠ¤"):
                arts = [a for a in crawl_naver_news(topic) if a['ë‚ ì§œ'] >= threshold]
                render_articles(arts)
