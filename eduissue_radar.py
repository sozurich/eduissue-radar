
import streamlit as st
import pandas as pd
import re
from collections import Counter
from datetime import datetime
import requests
import openai
from openai import OpenAI
import time
from gensim.summarization import summarize as gensim_summarize

# 1. Parse Kakao text
def parse_kakao_text(file):
    text = file.read().decode('utf-8')
    lines = text.splitlines()
    parsed = []
    current_date = None
    date_pattern = r'-{10,}\s*(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼.*?)\s*-{10,}'
    msg_pattern = r'\[(.*?)\] \[(ì˜¤ì „|ì˜¤í›„) (\d{1,2}:\d{2})\] (.+)'
    for line in lines:
        dm = re.match(date_pattern, line)
        if dm:
            current_date = dm.group(1)
            continue
        mm = re.match(msg_pattern, line)
        if mm and current_date:
            user, ampm, time_str, msg = mm.groups()
            hour, minute = map(int, time_str.split(':'))
            if ampm == 'ì˜¤í›„' and hour != 12:
                hour += 12
            timestamp = f"{hour:02}:{minute:02}"
            parsed.append({"ë‚ ì§œ": current_date, "ì‚¬ìš©ì": user, "ì‹œê°„": timestamp, "ë©”ì‹œì§€": msg})
    return pd.DataFrame(parsed)

# 2. Extract issues
issue_keywords = ["ë°°ì†¡","ì§€ì—°","ëˆ„ë½","ë¶ˆëŸ‰","ë¶€ì¡±","ì •ì‚°","ë°˜í’ˆ","ì¶”ê°€","ì˜¤ë¥˜"]
def extract_issues(df):
    msgs = df[df['ë©”ì‹œì§€'].str.contains('|'.join(issue_keywords))]
    words = ' '.join(msgs['ë©”ì‹œì§€'].tolist())
    nouns = re.findall(r'[\uAC00-\uD7A3]+', words)
    cnt = Counter(nouns)
    return msgs, cnt.most_common(10)

# 3. Naver OpenAPI crawl
def crawl_naver_openapi(query):
    client_id = st.secrets.get("NAVER_CLIENT_ID")
    client_secret = st.secrets.get("NAVER_CLIENT_SECRET")
    if not client_id or not client_secret:
        st.error("NAVER API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return []
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    params = {"query": query + " êµê³¼ì„œ", "display": 5, "sort": "date"}
    res = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
    if res.status_code != 200:
        st.error(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {res.status_code}")
        return []
    items = res.json().get("items", [])
    results = []
    for it in items:
        title = it.get("title","").replace("<b>","").replace("</b>","")
        link = it.get("originallink") or it.get("link")
        date_str = it.get("pubDate","")
        try:
            pub = datetime.strptime(date_str,'%a, %d %b %Y %H:%M:%S %z')
        except:
            pub = datetime.now()
        results.append({"ì œëª©": title, "ë§í¬": link, "ë‚ ì§œ": pub, "í‘œì‹œë‚ ì§œ": pub.strftime('%Y-%m-%d')})
    return results

# 4. GPT summary with caching/backoff
@st.cache_data(ttl=3600)
def summarize_gpt(messages):
    api_key = st.secrets.get("OPENAI_API_KEY")
    if not api_key:
        return "OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
    client = OpenAI(api_key=api_key)
    prompt = "ì•„ë˜ëŠ” êµê³¼ì„œ ê´€ë ¨ ë¯¼ì› ë©”ì‹œì§€ì…ë‹ˆë‹¤. ì£¼ìš” ì´ìŠˆì™€ ë¶„ìœ„ê¸°ë¥¼ 3~4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n\n" + "\n".join(messages)
    for i in range(3):
        try:
            resp = client.chat.completions.create(model="gpt-3.5-turbo",
                                                  messages=[{"role":"user","content":prompt}],
                                                  temperature=0.7)
            return resp.choices[0].message.content
        except Exception:
            time.sleep(2**i)
    return "ìš”ì²­ì´ ë§ì•„ ìš”ì•½ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."

# 5. Local gensim summary
@st.cache_data(ttl=3600)
def summarize_local(text, ratio):
    try:
        return gensim_summarize(text, ratio=ratio)
    except Exception:
        return "ë¡œì»¬ ìš”ì•½ì„ ìœ„í•œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

# 6. Streamlit UI
st.title("ğŸ“š EduIssue Radar")
st.markdown("êµê³¼ì„œ ë¯¼ì› ë©”ì‹œì§€ ë¶„ì„ ë° í‚¤ì›Œë“œ ë‰´ìŠ¤ / ìš”ì•½ ê¸°ëŠ¥")

uploaded = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ ì±„íŒ… .txt íŒŒì¼ ì—…ë¡œë“œ", type="txt")
if uploaded:
    df = parse_kakao_text(uploaded)
    df['ë‚ ì§œ'] = pd.to_datetime(
        df['ë‚ ì§œ'].str.extract(r'(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼)', expand=False),
        format='%Yë…„ %mì›” %dì¼', errors='coerce').dt.date
    sd, ed = st.date_input("ë¶„ì„ ê¸°ê°„ ì„ íƒ", [df['ë‚ ì§œ'].min(), df['ë‚ ì§œ'].max()])
    df_sel = df[(df['ë‚ ì§œ'] >= sd) & (df['ë‚ ì§œ'] <= ed)]
    iss_df, top = extract_issues(df_sel)

    tab1, tab2, tab3 = st.tabs(["ğŸ“Š ë¯¼ì› ë¶„ì„","ğŸ“° í‚¤ì›Œë“œ ë‰´ìŠ¤","ğŸ“ ìš”ì•½"])
    with tab1:
        st.success(f"{sd} ~ {ed} ë©”ì‹œì§€ {len(df_sel)}ê±´")
        st.write(iss_df[['ë‚ ì§œ','ì‹œê°„','ì‚¬ìš©ì','ë©”ì‹œì§€']])
        st.markdown("**ë¯¼ì› í‚¤ì›Œë“œ TOP10**")
        for i in range(0, len(top), 3):
            cols = st.columns(3)
            for j,(kw,cnt) in enumerate(top[i:i+3]):
                cols[j].markdown(f"- **{kw}** ({cnt}íšŒ)")
    with tab2:
        st.subheader("ğŸ” ì—°ê´€ ë‰´ìŠ¤")
        for kw,_ in top[:3]:
            with st.expander(f"{kw} ê´€ë ¨ ë‰´ìŠ¤"):
                arts = crawl_naver_openapi(kw)
                for art in arts:
                    st.markdown(f"- [{art['ì œëª©']}]({art['ë§í¬']}) ({art['í‘œì‹œë‚ ì§œ']})")
        st.subheader("ğŸ“š ì£¼ì œë³„ ì¶”ì²œ ë‰´ìŠ¤")
        extra_topics = ["êµê³¼ì„œ","AI ë””ì§€í„¸êµê³¼ì„œ","ë¹„ìƒêµìœ¡","ì²œì¬êµìœ¡","ë¯¸ë˜ì—”"]
        for topic in extra_topics:
            with st.expander(f"{topic} ê´€ë ¨ ë‰´ìŠ¤"):
                arts = crawl_naver_openapi(topic)
                for art in arts:
                    st.markdown(f"- [{art['ì œëª©']}]({art['ë§í¬']}) ({art['í‘œì‹œë‚ ì§œ']})")
    with tab3:
        msgs = df_sel['ë©”ì‹œì§€'].tolist()
        if msgs:
            st.subheader("ğŸ¤– GPT ìš”ì•½")
            if st.button("âœ… GPT ìš”ì•½ ìš”ì²­"):
                summary_gpt = summarize_gpt(tuple(msgs[-200:]))
                st.write(summary_gpt)
            st.markdown("---")
            st.subheader("ğŸ› ï¸ gensim ë¡œì»¬ ìš”ì•½")
            ratio = st.slider("ìš”ì•½ ë¹„ìœ¨", 0.05, 0.3, 0.1, 0.05)
            text = "\n".join(msgs)
            summary_local = summarize_local(text, ratio)
            st.write(summary_local)
        else:
            st.markdown("ë¶„ì„í•  ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
