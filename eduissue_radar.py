
import streamlit as st
import pandas as pd
import re
from collections import Counter
from datetime import datetime
import requests

# 1. Kakao í…ìŠ¤íŠ¸ íŒŒì‹±
def parse_kakao_text(file):
    text = file.read().decode('utf-8')
    lines = text.splitlines()
    parsed=[]
    current_date=None
    date_pattern=r'-{10,}\s*(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼.*?)\s*-{10,}'
    msg_pattern=r'\[(.*?)\] \[(ì˜¤ì „|ì˜¤í›„) (\d{1,2}:\d{2})\] (.+)'
    for line in lines:
        dm=re.match(date_pattern,line)
        if dm:
            current_date=dm.group(1)
            continue
        mm=re.match(msg_pattern,line)
        if mm and current_date:
            user,ampm,time_str,msg=mm.groups()
            hour,minute=map(int,time_str.split(':'))
            if ampm=='ì˜¤í›„' and hour!=12: hour+=12
            timestamp=f"{hour:02}:{minute:02}"
            parsed.append({"ë‚ ì§œ":current_date,"ì‚¬ìš©ì":user,"ì‹œê°„":timestamp,"ë©”ì‹œì§€":msg})
    return pd.DataFrame(parsed)

# 2. ë¯¼ì› í‚¤ì›Œë“œ ì¶”ì¶œ
issue_keywords=["ë°°ì†¡","ì§€ì—°","ëˆ„ë½","ë¶ˆëŸ‰","ë¶€ì¡±","ì •ì‚°","ë°˜í’ˆ","ì¶”ê°€","ì˜¤ë¥˜"]
def extract_issues(df):
    msgs=df[df['ë©”ì‹œì§€'].str.contains('|'.join(issue_keywords))]
    words=' '.join(msgs['ë©”ì‹œì§€'].tolist())
    nouns=re.findall(r'[\uAC00-\uD7A3]+',words)
    cnt=Counter(nouns)
    return msgs,cnt.most_common(10)

# 3. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
def crawl_naver_openapi(query):
    client_id=st.secrets.get('NAVER_CLIENT_ID')
    client_secret=st.secrets.get('NAVER_CLIENT_SECRET')
    if not client_id or not client_secret:
        st.error('NAVER API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.')
        return []
    headers={'X-Naver-Client-Id':client_id,'X-Naver-Client-Secret':client_secret}
    params={'query':query+' êµê³¼ì„œ','display':5,'sort':'date'}
    res=requests.get('https://openapi.naver.com/v1/search/news.json',headers=headers,params=params)
    if res.status_code!=200:
        st.error(f'ë„¤ì´ë²„ API ì˜¤ë¥˜: {res.status_code}')
        return []
    items=res.json().get('items',[])
    results=[]
    for it in items:
        title=it.get('title','').replace('<b>','').replace('</b>','')
        link=it.get('originallink') or it.get('link')
        date_str=it.get('pubDate','')
        try:
            pub=datetime.strptime(date_str,'%a, %d %b %Y %H:%M:%S %z')
        except:
            pub=datetime.now()
        results.append({'ì œëª©':title,'ë§í¬':link,'ë‚ ì§œ':pub,'í‘œì‹œë‚ ì§œ':pub.strftime('%Y-%m-%d')})
    return results

# 4. Local summarizer
def local_summarize(text,max_sentences=5):
    # split into sentences
    sents=re.split(r'(?<=[.!?])\s+|\n',text)
    # calculate word freq
    words=re.findall(r'[\wê°€-í£]+',text.lower())
    stop=set(['ì´ê±°','ê·¸ê±°','ì €í¬','í•©ë‹ˆë‹¤','ì…ë‹ˆë‹¤','ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ìš”','í•´ìš”'])
    freq=Counter(w for w in words if len(w)>1 and w not in stop)
    # score sentences
    scored=[]
    for i,s in enumerate(sents):
        w=re.findall(r'[\wê°€-í£]+',s.lower())
        score=sum(freq.get(wd,0) for wd in w)
        scored.append((score,i,s))
    top=sorted(scored,reverse=True)[:max_sentences]
    top_sorted=sorted(top,key=lambda x:x[1])
    summary=' '.join(s for _,_,s in top_sorted)
    return summary or 'ë¡œì»¬ ìš”ì•½ì„ ìœ„í•œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.'

# 5. UI
st.title('ğŸ“š EduIssue Radar')
st.markdown('êµê³¼ì„œ ë¯¼ì› ë©”ì‹œì§€ ë¶„ì„ + ë¡œì»¬ ìš”ì•½')

uploaded=st.file_uploader('ì¹´ì¹´ì˜¤í†¡ ì±„íŒ… .txt íŒŒì¼ ì—…ë¡œë“œ',type='txt')
if uploaded:
    df=parse_kakao_text(uploaded)
    df['ë‚ ì§œ']=pd.to_datetime(df['ë‚ ì§œ'].str.extract(r'(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼)',expand=False),format='%Yë…„ %mì›” %dì¼',errors='coerce').dt.date
    min_d,max_d=df['ë‚ ì§œ'].min(),df['ë‚ ì§œ'].max()
    st.markdown(f'**ë¶„ì„ ê°€ëŠ¥í•œ ë‚ ì§œ:** {min_d} ~ {max_d}')
    sd,ed=st.date_input('ë¶„ì„ ê¸°ê°„ ì„ íƒ',[min_d,max_d])
    df_sel=df[(df['ë‚ ì§œ']>=sd)&(df['ë‚ ì§œ']<=ed)]
    iss_df,top=extract_issues(df_sel)

    tab1,tab2,tab3=st.tabs(['ğŸ“Š ë¯¼ì› ë¶„ì„','ğŸ“° í‚¤ì›Œë“œ ë‰´ìŠ¤','ğŸ“ ìš”ì•½'])
    with tab1:
        st.success(f'{sd} ~ {ed} ë©”ì‹œì§€ {len(df_sel)}ê±´')
        st.write(iss_df[['ë‚ ì§œ','ì‹œê°„','ì‚¬ìš©ì','ë©”ì‹œì§€']])
        st.markdown('**ë¯¼ì› í‚¤ì›Œë“œ TOP10**')
        for i in range(0,len(top),3):
            cols=st.columns(3)
            for j,(kw,cnt) in enumerate(top[i:i+3]):
                cols[j].markdown(f'- **{kw}** ({cnt}íšŒ)')
    with tab2:
        st.subheader('ğŸ” ì—°ê´€ ë‰´ìŠ¤')
        for kw,_ in top[:3]:
            with st.expander(f'{kw} ê´€ë ¨ ë‰´ìŠ¤'):
                arts=crawl_naver_openapi(kw)
                for a in arts:
                    st.markdown(f"- [{a['ì œëª©']}]({a['ë§í¬']}) ({a['í‘œì‹œë‚ ì§œ']})")
        st.subheader('ğŸ“š ì£¼ì œë³„ ì¶”ì²œ ë‰´ìŠ¤')
        extras=['êµê³¼ì„œ','AI ë””ì§€í„¸êµê³¼ì„œ','ë¹„ìƒêµìœ¡','ì²œì¬êµìœ¡','ë¯¸ë˜ì—”']
        for topic in extras:
            with st.expander(f'{topic} ê´€ë ¨ ë‰´ìŠ¤'):
                arts=crawl_naver_openapi(topic)
                for a in arts:
                    st.markdown(f"- [{a['ì œëª©']}]({a['ë§í¬']}) ({a['í‘œì‹œë‚ ì§œ']})")
    with tab3:
        st.subheader('ğŸ“ ë¡œì»¬ ìš”ì•½')
        msgs=df_sel['ë©”ì‹œì§€'].tolist()
        text=' '.join(msgs)
        count=st.slider('ìš”ì•½ ë¬¸ì¥ ìˆ˜',1,10,5)
        if st.button('âœ… ìš”ì•½ ì‹¤í–‰'):
            summary=local_summarize(text,count)
            st.write(summary)
        else:
            st.markdown('ë²„íŠ¼ì„ ëˆŒëŸ¬ ìš”ì•½ì„ ì‹¤í–‰í•˜ì„¸ìš”.')
