
import streamlit as st
import pandas as pd
import re
from collections import Counter
from datetime import datetime
import requests
import openai
from openai import OpenAI
import time
from gensim.summarization import summarize as gensim_summarize

# Initialize session state for summary request
if 'gpt_requested' not in st.session_state:
    st.session_state['gpt_requested'] = False

# 1. Kakao í…ìŠ¤íŠ¸ íŒŒì‹±
def parse_kakao_text(file):
    text = file.read().decode('utf-8')
    lines = text.splitlines()
    parsed = []
    current_date = None
    date_pattern = r'-{10,}\s*(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼.*?)\s*-{10,}'
    msg_pattern = r'\[(.*?)\] \[(ì˜¤ì „|ì˜¤í›„) (\d{1,2}:\d{2})\] (.+)'
    for line in lines:
        dm = re.match(date_pattern, line)
        if dm:
            current_date = dm.group(1)
            continue
        mm = re.match(msg_pattern, line)
        if mm and current_date:
            user, ampm, time_str, msg = mm.groups()
            hour, minute = map(int, time_str.split(':'))
            if ampm == 'ì˜¤í›„' and hour != 12:
                hour += 12
            timestamp = f"{hour:02}:{minute:02}"
            parsed.append({"ë‚ ì§œ": current_date, "ì‚¬ìš©ì": user, "ì‹œê°„": timestamp, "ë©”ì‹œì§€": msg})
    return pd.DataFrame(parsed)

# 2. ë¯¼ì› í‚¤ì›Œë“œ ì¶”ì¶œ
issue_keywords = ["ë°°ì†¡","ì§€ì—°","ëˆ„ë½","ë¶ˆëŸ‰","ë¶€ì¡±","ì •ì‚°","ë°˜í’ˆ","ì¶”ê°€","ì˜¤ë¥˜"]
def extract_issues(df):
    msgs = df[df['ë©”ì‹œì§€'].str.contains('|'.join(issue_keywords))]
    words = ' '.join(msgs['ë©”ì‹œì§€'].tolist())
    nouns = re.findall(r'[\uAC00-\uD7A3]+', words)
    cnt = Counter(nouns)
    return msgs, cnt.most_common(10)

# 3. ë„¤ì´ë²„ OpenAPI ë‰´ìŠ¤ í¬ë¡¤ë§
def crawl_naver_openapi(query):
    client_id = st.secrets.get("NAVER_CLIENT_ID")
    client_secret = st.secrets.get("NAVER_CLIENT_SECRET")
    if not client_id or not client_secret:
        st.error("NAVER API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return []
    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
    params = {"query": query + " êµê³¼ì„œ", "display": 5, "sort": "date"}
    res = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
    if res.status_code != 200:
        st.error(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {res.status_code}")
        return []
    items = res.json().get("items", [])
    results = []
    for it in items:
        title = it.get("title","").replace("<b>","").replace("</b>","")
        link = it.get("originallink") or it.get("link")
        date_str = it.get("pubDate","")
        try:
            pub = datetime.strptime(date_str,'%a, %d %b %Y %H:%M:%S %z')
        except:
            pub = datetime.now()
        results.append({"ì œëª©": title, "ë§í¬": link, "ë‚ ì§œ": pub, "í‘œì‹œë‚ ì§œ": pub.strftime('%Y-%m-%d')})
    return results

# 4. GPT ìš”ì•½ í•¨ìˆ˜ with backoff
def summarize_with_gpt(messages):
    api_key = st.secrets.get("OPENAI_API_KEY")
    if not api_key:
        st.error("OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return ""
    client = OpenAI(api_key=api_key)
    prompt = "ì•„ë˜ëŠ” êµê³¼ì„œ ê´€ë ¨ ë¯¼ì› ë©”ì‹œì§€ ëŒ€í™”ì…ë‹ˆë‹¤. ì£¼ìš” ì´ìŠˆì™€ ë¶„ìœ„ê¸°ë¥¼ 3~4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n\n" + "\n".join(messages)
    for i in range(3):
        try:
            resp = client.chat.completions.create(model="gpt-3.5-turbo", messages=[{"role":"user","content":prompt}], temperature=0.7)
            return resp.choices[0].message.content
        except Exception:
            time.sleep(2**i)
    st.warning("ìš”ì²­ì´ ë§ì•„ ìš”ì•½ì„ ì ì‹œ í›„ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    return ""

@st.cache_data(ttl=3600)
def summarize_cached(messages):
    return summarize_with_gpt(messages)

@st.cache_data(ttl=3600)
def summarize_gensim(text, ratio):
    try:
        return gensim_summarize(text, ratio=ratio)
    except ValueError:
        return "ë¡œì»¬ ìš”ì•½ì„ ìœ„í•œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

# 5. Streamlit UI
st.title("ğŸ“š EduIssue Radar")
st.markdown("êµê³¼ì„œ ë¯¼ì› ë©”ì‹œì§€ ë¶„ì„ + ë‰´ìŠ¤ ìš”ì•½ (GPT & ë¡œì»¬)")

uploaded = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ ì±„íŒ… .txt íŒŒì¼ ì—…ë¡œë“œ", type="txt")
if uploaded:
    df = parse_kakao_text(uploaded)
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'].str.extract(r'(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼)', expand=False), format='%Yë…„ %mì›” %dì¼', errors='coerce').dt.date
    sd, ed = st.date_input("ë¶„ì„ ê¸°ê°„ ì„ íƒ", [df['ë‚ ì§œ'].min(), df['ë‚ ì§œ'].max()])
    df_sel = df[(df['ë‚ ì§œ']>=sd)&(df['ë‚ ì§œ']<=ed)]
    iss_df, top = extract_issues(df_sel)

    tab1, tab2, tab3 = st.tabs(["ğŸ“Š ë¯¼ì› ë¶„ì„","ğŸ“° ì—°ê´€ ë‰´ìŠ¤","ğŸ“ ìš”ì•½"])
    with tab1:
        st.write(iss_df[['ë‚ ì§œ','ì‹œê°„','ì‚¬ìš©ì','ë©”ì‹œì§€']])
        st.markdown("**ë¯¼ì› í‚¤ì›Œë“œ TOP10**")
        for i in range(0,len(top),3):
            cols = st.columns(3)
            for j,(kw,cnt) in enumerate(top[i:i+3]):
                cols[j].markdown(f"- **{kw}** ({cnt}íšŒ)")
    with tab2:
        for kw,_ in top[:3]:
            with st.expander(f"ğŸ” {kw} ê´€ë ¨ ë‰´ìŠ¤"):
                arts = crawl_naver_openapi(kw)
                for art in arts:
                    st.markdown(f"- [{art['ì œëª©']}]({art['ë§í¬']}) ({art['í‘œì‹œë‚ ì§œ']})")
    with tab3:
        msgs = df_sel['ë©”ì‹œì§€'].tolist()
        text = "\n".join(msgs)
        st.subheader("GPT ìš”ì•½")
        if st.button("âœ… GPT ìš”ì•½ ìš”ì²­"):
            summary_gpt = summarize_cached(tuple(msgs[-200:]))
            st.write(summary_gpt)
        st.markdown("---")
        st.subheader("ë¡œì»¬ ìš”ì•½ (Gensim)")
        ratio = st.slider("ìš”ì•½ ë¹„ìœ¨", 0.05, 0.3, 0.1, 0.05)
        summary_local = summarize_gensim(text, ratio)
        st.write(summary_local)
