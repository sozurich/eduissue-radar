
import streamlit as st
import pandas as pd
import re
from collections import Counter
from datetime import datetime
from email.utils import parsedate_to_datetime
import requests
from bs4 import BeautifulSoup

def parse_kakao_text(file):
    text = file.read().decode('utf-8')
    lines = text.splitlines()
    parsed = []
    current_date = None
    date_pattern = r'-{10,}\s*(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼.*?)\s*-{10,}'
    msg_pattern = r'\[(.*?)\] \[(ì˜¤ì „|ì˜¤í›„) (\d{1,2}:\d{2})\] (.+)'

    for line in lines:
        date_match = re.match(date_pattern, line)
        if date_match:
            current_date = date_match.group(1)
            continue

        msg_match = re.match(msg_pattern, line)
        if msg_match and current_date:
            user, ampm, time, msg = msg_match.groups()
            hour, minute = map(int, time.split(':'))
            if ampm == 'ì˜¤í›„' and hour != 12:
                hour += 12
            timestamp = f"{hour:02}:{minute:02}"
            parsed.append({"ë‚ ì§œ": current_date, "ì‚¬ìš©ìž": user, "ì‹œê°„": timestamp, "ë©”ì‹œì§€": msg})

    return pd.DataFrame(parsed)

issue_keywords = ["ë°°ì†¡", "ì§€ì—°", "ëˆ„ë½", "ë¶ˆëŸ‰", "ë¶€ì¡±", "ì •ì‚°", "ë°˜í’ˆ", "ì¶”ê°€", "ì˜¤ë¥˜"]
emotion_keywords = ["ë©˜ë¶•", "ëª¨ë¥´ê² ", "ì–´ë µ", "ë‹µë‹µ", "ë³µìž¡", "ë¯¸ì¹˜ê² ", "íœ´ì§", "íž˜ë“¤", "ìŠ¤íŠ¸ë ˆìŠ¤", "ì—‰ë§"]

def extract_issues(df):
    issue_msgs = df[df['ë©”ì‹œì§€'].str.contains('|'.join(issue_keywords))]
    all_words = ' '.join(issue_msgs['ë©”ì‹œì§€'].tolist())
    nouns = re.findall(r'[\uAC00-\uD7A3]+', all_words)
    count = Counter(nouns)
    return issue_msgs, count.most_common(10)

def extract_emotions(df):
    emotion_msgs = df[df['ë©”ì‹œì§€'].str.contains('|'.join(emotion_keywords))]
    all_words = ' '.join(emotion_msgs['ë©”ì‹œì§€'].tolist())
    nouns = re.findall(r'[\uAC00-\uD7A3]+', all_words)
    count = Counter(nouns)
    return emotion_msgs, count.most_common(10)

def crawl_google_news(query):
    url = f"https://news.google.com/rss/search?q={query}+êµê³¼ì„œ&hl=ko&gl=KR&ceid=KR:ko"
    res = requests.get(url)
    soup = BeautifulSoup(res.content, 'html.parser')
    items = soup.find_all('item')
    results = []
    seen_titles = set()
    for item in items:
        title = item.title.text
        if title in seen_titles:
            continue
        seen_titles.add(title)
        description_html = item.description.text
        soup_desc = BeautifulSoup(description_html, 'html.parser')
        link_tag = soup_desc.find('a')
        original_link = link_tag['href'] if link_tag else item.link.text
        if item.pubDate:
            pub_date = parsedate_to_datetime(item.pubDate.text)
            display_date = pub_date.strftime('%Y-%m-%d')
        else:
            pub_date = datetime.now()
            display_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
        results.append({
            "ì œëª©": title,
            "ë§í¬": original_link,
            "ë‚ ì§œ": pub_date,
            "í‘œì‹œë‚ ì§œ": display_date
        })
    results.sort(key=lambda x: x['ë‚ ì§œ'], reverse=True)
    return results

st.title("ðŸ“š EduIssue Radar")
st.markdown("êµê³¼ì„œ ë¯¼ì› ë©”ì‹œì§€ + ê°ì • ê°ì§€ + ë‰´ìŠ¤ ìš”ì•½ ë¶„ì„ê¸°")

uploaded_file = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ ì±„íŒ… .txt íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="txt")

if uploaded_file:
    df = parse_kakao_text(uploaded_file)
    df['ë‚ ì§œ'] = df['ë‚ ì§œ'].fillna(method='ffill')
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'].str.extract(r'(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼)')[0], format="%Yë…„ %mì›” %dì¼")

    min_date = df['ë‚ ì§œ'].min()
    max_date = df['ë‚ ì§œ'].max()

    st.markdown(f"**ë¶„ì„ ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„:** {min_date.date()} ~ {max_date.date()}")
    start_date, end_date = st.date_input("ë¶„ì„í•  ê¸°ê°„ì„ ì„ íƒí•˜ì„¸ìš”", [min_date, max_date])

    df_selected = df[(df['ë‚ ì§œ'] >= pd.to_datetime(start_date)) & (df['ë‚ ì§œ'] <= pd.to_datetime(end_date))]

    tab1, tab2 = st.tabs(["ðŸ“Š ë¯¼ì› ë° ê°ì • ë¶„ì„", "ðŸ“° ë‰´ìŠ¤ ìš”ì•½"])

    with tab1:
        st.success(f"{start_date} ~ {end_date} ê¸°ê°„ì˜ ë©”ì‹œì§€ {len(df_selected)}ê±´ ë¶„ì„ ì¤‘...")

        issue_df, top_issue_keywords = extract_issues(df_selected)
        st.subheader("ðŸš¨ ë¯¼ì› ë©”ì‹œì§€ ê°ì§€")
        st.write(issue_df[['ë‚ ì§œ', 'ì‹œê°„', 'ì‚¬ìš©ìž', 'ë©”ì‹œì§€']])
        st.markdown("**ë¯¼ì› í‚¤ì›Œë“œ TOP10**")
        
    for i in range(0, len(top_issue_keywords), 3):
        cols = st.columns(3)
        row_items = top_issue_keywords[i:i+3]
        for j in range(len(row_items)):
            word, freq = row_items[j]
            cols[j].markdown(f"- **{word}** ({freq}íšŒ)")
    
            st.write(f"- {word} ({freq}íšŒ)")

        emotion_df, top_emotion_keywords = extract_emotions(df_selected)
        st.subheader("ðŸ˜¥ ê°ì • í‘œí˜„ ê°ì§€")
        st.write(emotion_df[['ë‚ ì§œ', 'ì‹œê°„', 'ì‚¬ìš©ìž', 'ë©”ì‹œì§€']])
        st.markdown("**ê°ì • í‚¤ì›Œë“œ TOP10**")
        
    positive_words = ["ì¢‹ì•„ìš”", "ê°ì‚¬", "ë„ì›€", "ìž˜ë", "ë‹¤í–‰"]
    negative_words = ["ë©˜ë¶•", "ì–´ë µ", "ë‹µë‹µ", "ë¯¸ì¹˜ê² ", "íž˜ë“¤"]

    col_pos, col_neg = st.columns(2)
    col_pos.markdown("**ðŸ˜Š ê¸ì • í‘œí˜„ (ì˜ˆì‹œ)**")
    for word in positive_words:
        col_pos.write(f"- {word}")

    col_neg.markdown("**ðŸ˜¥ ë¶€ì • í‘œí˜„ (ì˜ˆì‹œ)**")
    for word in negative_words:
        col_neg.write(f"- {word}")

    st.markdown("**ê°ì • í‚¤ì›Œë“œ TOP10**")
    for i in range(0, len(top_emotion_keywords), 3):
        cols = st.columns(3)
        for j, (word, freq) in enumerate(top_emotion_keywords[i:i+3]):
            if j < len(cols):
                cols[j].markdown(f"- **{word}** ({freq}íšŒ)")
    
            st.write(f"- {word} ({freq}íšŒ)")

    with tab2:
        st.subheader("ðŸ“° ë‰´ìŠ¤ ìš”ì•½")
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("### ðŸ“Œ ì—°ê´€ ë‰´ìŠ¤ ê¸°ì‚¬")
            for word, _ in top_issue_keywords[:3]:
                with st.expander(f"ðŸ”Ž {word} ê´€ë ¨ ë‰´ìŠ¤"):
                    articles = crawl_google_news(word)
                    for article in articles[:5]:
                        st.markdown(f"**{article['ì œëª©']}** ({article['í‘œì‹œë‚ ì§œ']})")
                        st.link_button("ðŸ”— ë‰´ìŠ¤ ë³´ëŸ¬ê°€ê¸°", url=article["ë§í¬"])

        with col2:
            st.markdown("### ðŸ“š ì£¼ì œë³„ ì¶”ì²œ ë‰´ìŠ¤")
            extra_topics = ["êµê³¼ì„œ", "AI ë””ì§€í„¸êµê³¼ì„œ", "ë¹„ìƒêµìœ¡", "ì²œìž¬êµìœ¡", "ì²œìž¬êµê³¼ì„œ", "ë¯¸ëž˜ì—”", "ì•„ì´ìŠ¤í¬ë¦¼ë¯¸ë””ì–´", "ë™ì•„ì¶œíŒ", "ì§€í•™ì‚¬"]
            for topic in extra_topics:
                with st.expander(f"ðŸ“˜ {topic} ê´€ë ¨ ë‰´ìŠ¤"):
                    articles = crawl_google_news(topic)
                    for article in articles[:5]:
                        st.markdown(f"**{article['ì œëª©']}** ({article['í‘œì‹œë‚ ì§œ']})")
                        st.link_button("ðŸ”— ë‰´ìŠ¤ ë³´ëŸ¬ê°€ê¸°", url=article["ë§í¬"])
