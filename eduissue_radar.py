
# EduIssue Radar - Streamlit μ•± (κΈ°κ°„ λ¶„μ„ κΈ°λ¥ ν¬ν•¨)

import streamlit as st
import pandas as pd
import re
from collections import Counter
from datetime import datetime
import requests
from bs4 import BeautifulSoup

# 1. ν…μ¤νΈ νμΌ νμ‹± ν•¨μ (μ¤„ λ‹¨μ„ λ‚ μ§ λ§¤ν•‘)
def parse_kakao_text(file):
    text = file.read().decode('utf-8')
    lines = text.splitlines()
    parsed = []
    current_date = None
    date_pattern = r'-{10,}\s*(\d{4}λ…„ \d{1,2}μ›” \d{1,2}μΌ.*?)\s*-{10,}'
    msg_pattern = r'\[(.*?)\] \[(μ¤μ „|μ¤ν›„) (\d{1,2}:\d{2})\] (.+)'

    for line in lines:
        date_match = re.match(date_pattern, line)
        if date_match:
            current_date = date_match.group(1)
            continue

        msg_match = re.match(msg_pattern, line)
        if msg_match and current_date:
            user, ampm, time, msg = msg_match.groups()
            hour, minute = map(int, time.split(':'))
            if ampm == 'μ¤ν›„' and hour != 12:
                hour += 12
            timestamp = f"{hour:02}:{minute:02}"
            parsed.append({"λ‚ μ§": current_date, "μ‚¬μ©μ": user, "μ‹κ°„": timestamp, "λ©”μ‹μ§€": msg})

    return pd.DataFrame(parsed)

# 2. ν‚¤μ›λ“ κΈ°λ° λ―Όμ› λ©”μ‹μ§€ ν•„ν„°λ§
issue_keywords = ["λ°°μ†΅", "μ§€μ—°", "λ„λ½", "λ¶λ‰", "λ¶€μ΅±", "μ •μ‚°", "λ°ν’", "μ¶”κ°€", "μ¤λ¥"]

def extract_issues(df):
    issue_msgs = df[df['λ©”μ‹μ§€'].str.contains('|'.join(issue_keywords))]
    all_words = ' '.join(issue_msgs['λ©”μ‹μ§€'].tolist())
    nouns = re.findall(r'[\uAC00-\uD7A3]+', all_words)
    count = Counter(nouns)
    return issue_msgs, count.most_common(10)

# 3. λ‰΄μ¤ ν¬λ΅¤λ¬ ν•¨μ (λ„¤μ΄λ²„ λ‰΄μ¤ κ²€μƒ‰)
def crawl_news(query):
    headers = {"User-Agent": "Mozilla/5.0"}
    url = f"https://search.naver.com/search.naver?where=news&query={query}"
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, 'html.parser')
    news_items = soup.select(".list_news .news_area")
    seen_titles = set()
    results = []
    for item in news_items:
        title_tag = item.select_one(".news_tit")
        if title_tag:
            title = title_tag.text.strip()
            if title in seen_titles:
                continue
            seen_titles.add(title)
            link = title_tag['href']
            press = item.select_one(".info_group span").text if item.select_one(".info_group span") else "μ–Έλ΅ μ‚¬ λ―Έν™•μΈ"
            date_tag = item.select_one(".info_group span:nth-of-type(2)")
            pub_date = date_tag.text if date_tag else "λ‚ μ§ λ―Έν™•μΈ"
            results.append({"μ λ©": title, "λ§ν¬": link, "μ–Έλ΅ μ‚¬": press, "λ‚ μ§": pub_date})
        if len(results) >= 5:
            break
    return results

# 4. Streamlit μΈν„°νμ΄μ¤
st.title("π“ EduIssue Radar")
st.markdown("κµκ³Όμ„ λ―Όμ› λ©”μ‹μ§€ + λ‰΄μ¤ ν‚¤μ›λ“ ν†µν•© λ¶„μ„κΈ°")

uploaded_file = st.file_uploader("μΉ΄μΉ΄μ¤ν†΅ μ±„ν… .txt νμΌμ„ μ—…λ΅λ“ν•μ„Έμ”", type="txt")

if uploaded_file:
    df = parse_kakao_text(uploaded_file)
    df['λ‚ μ§'] = df['λ‚ μ§'].fillna(method='ffill')
    df['λ‚ μ§'] = pd.to_datetime(df['λ‚ μ§'].str.extract(r'(\d{4}λ…„ \d{1,2}μ›” \d{1,2}μΌ)')[0], format="%Yλ…„ %mμ›” %dμΌ")

    min_date = df['λ‚ μ§'].min()
    max_date = df['λ‚ μ§'].max()

    st.markdown(f"**λ¶„μ„ κ°€λ¥ν• λ‚ μ§ λ²”μ„:** {min_date.date()} ~ {max_date.date()}")
    start_date, end_date = st.date_input("λ¶„μ„ν•  κΈ°κ°„μ„ μ„ νƒν•μ„Έμ”", [min_date, max_date])

    df_selected = df[(df['λ‚ μ§'] >= pd.to_datetime(start_date)) & (df['λ‚ μ§'] <= pd.to_datetime(end_date))]

    st.success(f"{start_date} ~ {end_date} κΈ°κ°„μ λ©”μ‹μ§€ {len(df_selected)}κ±΄ λ¶„μ„ μ¤‘...")

    issue_df, top_keywords = extract_issues(df_selected)
    st.subheader("π” λ―Όμ› λ©”μ‹μ§€ μ”μ•½")
    st.write(issue_df[['λ‚ μ§', 'μ‹κ°„', 'μ‚¬μ©μ', 'λ©”μ‹μ§€']])

    st.subheader("π”¥ μμ£Ό μ–ΈκΈ‰λ ν‚¤μ›λ“")
    for word, freq in top_keywords:
        st.write(f"- {word} ({freq}ν)")

    st.subheader("π“° λ‰΄μ¤ μ”μ•½")
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### π“ μ—°κ΄€ λ‰΄μ¤ κΈ°μ‚¬")
        for word, _ in top_keywords[:3]:
            with st.expander(f"π” {word} κ΄€λ ¨ λ‰΄μ¤"):
                articles = crawl_news(word + " κµκ³Όμ„")
                for article in articles:
                    st.markdown(f"- [{article['μ λ©']}]({article['λ§ν¬']})  
  <{article['μ–Έλ΅ μ‚¬']} | {article['λ‚ μ§']}>")

    with col2:
        st.markdown("### π“ μ£Όμ λ³„ μ¶”μ² λ‰΄μ¤")
        extra_topics = ["κµκ³Όμ„", "AI λ””μ§€ν„Έκµκ³Όμ„", "λΉ„μƒκµμ΅", "μ²μ¬κµμ΅", "μ²μ¬κµκ³Όμ„", "λ―Έλμ—”", "μ•„μ΄μ¤ν¬λ¦Όλ―Έλ””μ–΄", "λ™μ•„μ¶ν", "μ§€ν•™μ‚¬"]
        for topic in extra_topics:
            with st.expander(f"π“ {topic} κ΄€λ ¨ λ‰΄μ¤"):
                articles = crawl_news(topic)
                for article in articles:
                    st.markdown(f"- [{article['μ λ©']}]({article['λ§ν¬']})  
  <{article['μ–Έλ΅ μ‚¬']} | {article['λ‚ μ§']}>")
