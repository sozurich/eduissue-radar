# EduIssue Radar - Streamlit ì•± (ê¸°ê°„ ë¶„ì„ ê¸°ëŠ¥ í¬í•¨)

import streamlit as st
import pandas as pd
import re
from collections import Counter, defaultdict
from datetime import datetime
import requests
from bs4 import BeautifulSoup

# 1. í…ìŠ¤íŠ¸ íŒŒì¼ íŒŒì‹± í•¨ìˆ˜ (ì¤„ ë‹¨ìœ„ ë‚ ì§œ ë§¤í•‘)
def parse_kakao_text(file):
    text = file.read().decode('utf-8')
    lines = text.splitlines()
    parsed = []
    current_date = None
    date_pattern = r'-{10,}\s*(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼.*?)\s*-{10,}'
    msg_pattern = r'\[(.*?)\] \[(ì˜¤ì „|ì˜¤í›„) (\d{1,2}:\d{2})\] (.+)'

    for line in lines:
        date_match = re.match(date_pattern, line)
        if date_match:
            current_date = date_match.group(1)
            continue

        msg_match = re.match(msg_pattern, line)
        if msg_match and current_date:
            user, ampm, time, msg = msg_match.groups()
            hour, minute = map(int, time.split(':'))
            if ampm == 'ì˜¤í›„' and hour != 12:
                hour += 12
            timestamp = f"{hour:02}:{minute:02}"
            parsed.append({"ë‚ ì§œ": current_date, "ì‚¬ìš©ìž": user, "ì‹œê°„": timestamp, "ë©”ì‹œì§€": msg})

    return pd.DataFrame(parsed)

# 2. í‚¤ì›Œë“œ ê¸°ë°˜ ë¯¼ì› ë©”ì‹œì§€ í•„í„°ë§
issue_keywords = ["ë°°ì†¡", "ì§€ì—°", "ëˆ„ë½", "ë¶ˆëŸ‰", "ë¶€ì¡±", "ì •ì‚°", "ë°˜í’ˆ", "ì¶”ê°€", "ì˜¤ë¥˜", "êµê³¼ì„œ", "ì£¼ë¬¸", "ì„ ì •","ì¶œíŒì‚¬","ë°œí–‰ì‚¬","ì§œì¦"]

def extract_issues(df):
    issue_msgs = df[df['ë©”ì‹œì§€'].str.contains('|'.join(issue_keywords))]
    all_words = ' '.join(issue_msgs['ë©”ì‹œì§€'].tolist())
    nouns = re.findall(r'[\uAC00-\uD7A3]+', all_words)
    count = Counter(nouns)
    return issue_msgs, count.most_common(10)

# 3. ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í•¨ìˆ˜ (ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰)
def crawl_news(query):
    headers = {"User-Agent": "Mozilla/5.0"}
    url = f"https://search.naver.com/search.naver?where=news&query={query}"
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, 'html.parser')
    news_items = soup.select(".list_news .news_area")
    seen_titles = set()
    results = []
    for item in news_items:
        title_tag = item.select_one(".news_tit")
        if title_tag:
            title = title_tag.text.strip()
            if title in seen_titles:
                continue
            seen_titles.add(title)
            link = title_tag['href']
            press = item.select_one(".info_group span").text if item.select_one(".info_group span") else "ì–¸ë¡ ì‚¬ ë¯¸í™•ì¸"
            date_tags = item.select(".info_group span")
            pub_date = "ë‚ ì§œ ë¯¸í™•ì¸"
            for tag in date_tags:
                if any(keyword in tag.text for keyword in ["ë¶„ ì „", "ì‹œê°„ ì „", "ì¼ ì „", "202", "203"]):
                    pub_date = tag.text
                    break
            results.append({"ì œëª©": title, "ë§í¬": link, "ì–¸ë¡ ì‚¬": press, "ë‚ ì§œ": pub_date})
        if len(results) >= 5:
            break
    return results

# 4. Streamlit ì¸í„°íŽ˜ì´ìŠ¤
st.title("ðŸ“š EduIssue Radar")
st.markdown("êµê³¼ì„œ ë¯¼ì› ë©”ì‹œì§€ + ë‰´ìŠ¤ í‚¤ì›Œë“œ í†µí•© ë¶„ì„ê¸°")

uploaded_file = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ ì±„íŒ… .txt íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="txt")

if uploaded_file:
    df = parse_kakao_text(uploaded_file)
    df['ë‚ ì§œ'] = df['ë‚ ì§œ'].fillna(method='ffill')
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'].str.extract(r'(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼)')[0], format="%Yë…„ %mì›” %dì¼")

    min_date = df['ë‚ ì§œ'].min()
    max_date = df['ë‚ ì§œ'].max()

    st.markdown(f"**ë¶„ì„ ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„:** {min_date.date()} ~ {max_date.date()}")
    start_date, end_date = st.date_input("ë¶„ì„í•  ê¸°ê°„ì„ ì„ íƒí•˜ì„¸ìš”", [min_date, max_date])

    df_selected = df[(df['ë‚ ì§œ'] >= pd.to_datetime(start_date)) & (df['ë‚ ì§œ'] <= pd.to_datetime(end_date))]

    st.success(f"{start_date} ~ {end_date} ê¸°ê°„ì˜ ë©”ì‹œì§€ {len(df_selected)}ê±´ ë¶„ì„ ì¤‘...")

    issue_df, top_keywords = extract_issues(df_selected)

    st.subheader("ðŸ” ë¯¼ì› ë©”ì‹œì§€ ìš”ì•½")
    st.write(issue_df[['ë‚ ì§œ', 'ì‹œê°„', 'ì‚¬ìš©ìž', 'ë©”ì‹œì§€']])

    st.subheader("ðŸ”¥ ìžì£¼ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ")
    groupings = {
        'ë°°ì†¡ ê´€ë ¨': ['ë°°ì†¡', 'ë„ì°©', 'ë°°ë‹¬', 'íƒë°°'],
        'ì§€ì—°/ëˆ„ë½': ['ì§€ì—°', 'ëˆ„ë½', 'ì—°ê¸°'],
        'ë¶ˆëŸ‰/ì˜¤ë¥˜': ['ë¶ˆëŸ‰', 'ì˜¤ë¥˜', 'ê³ ìž¥'],
        'ì •ì‚°/ë°˜í’ˆ': ['ì •ì‚°', 'ë°˜í’ˆ', 'í™˜ë¶ˆ'],
        'ê¸°íƒ€': []
    }

    summary = defaultdict(list)
    for word, freq in top_keywords:
        categorized = False
        for topic, keywords in groupings.items():
            if any(k in word for k in keywords):
                summary[topic].append((word, freq))
                categorized = True
                break
        if not categorized:
            summary['ê¸°íƒ€'].append((word, freq))

    example_sentences = {
        'ë°°ì†¡ ê´€ë ¨': ["ë°°ì†¡ì´ ì•„ì§ ë„ì°©í•˜ì§€ ì•Šì•˜ì–´ìš”", "íƒë°°ê°€ ì˜¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"],
        'ì§€ì—°/ëˆ„ë½': ["êµê³¼ì„œê°€ ëˆ„ë½ë˜ì—ˆì–´ìš”", "ë°°ì†¡ì´ ë„ˆë¬´ ì§€ì—°ë¼ìš”"],
        'ë¶ˆëŸ‰/ì˜¤ë¥˜': ["ì¸ì‡„ ì˜¤ë¥˜ê°€ ìžˆì–´ìš”", "ë¶ˆëŸ‰ êµê³¼ì„œê°€ ì™”ì–´ìš”"],
        'ì •ì‚°/ë°˜í’ˆ': ["ì •ì‚°ì´ ì•ˆ ëì–´ìš”", "ë°˜í’ˆí•˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"],
        'ê¸°íƒ€': ["ê¸°íƒ€ ì´ìŠˆê°€ ìžˆìŠµë‹ˆë‹¤"]
    }

    for topic, words in summary.items():
        st.markdown(f"**ðŸ—‚ {topic}**")
        sentiment_dict = {
            'ë°°ì†¡': 'ë¶€ì •', 'ì§€ì—°': 'ë¶€ì •', 'ëˆ„ë½': 'ë¶€ì •', 'ì˜¤ë¥˜': 'ë¶€ì •', 'ë¶ˆëŸ‰': 'ë¶€ì •',
            'ì§œì¦': 'ë¶€ì •', 'ë¶ˆíŽ¸': 'ë¶€ì •', 'ëŠ¦ìŒ': 'ë¶€ì •',
            'ë§Œì¡±': 'ê¸ì •', 'ë¹ ë¦„': 'ê¸ì •', 'ì¢‹ì•„ìš”': 'ê¸ì •', 'ì •í™•': 'ê¸ì •', 'ê°ì‚¬': 'ê¸ì •'
        }
        for word, freq in words:
            sentiment = sentiment_dict.get(word, 'ì¤‘ë¦½')
            st.write(f"- {word} ({freq}íšŒ) [{sentiment}]")
        with st.expander("ðŸ’¬ ëŒ€í‘œ ë¯¼ì› ì˜ˆì‹œ ë³´ê¸°"):
            for sentence in example_sentences.get(topic, []):
                st.markdown(f"â€¢ {sentence}")

    # ë‰´ìŠ¤ ìš”ì•½
    st.subheader("ðŸ“° ë‰´ìŠ¤ ìš”ì•½")
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### ðŸ“Œ ì—°ê´€ ë‰´ìŠ¤ ê¸°ì‚¬")
        for word, _ in top_keywords[:3]:
            with st.expander(f"ðŸ”Ž {word} ê´€ë ¨ ë‰´ìŠ¤"):
                articles = crawl_news(word + " êµê³¼ì„œ")
                for article in articles:
                    st.markdown(
                        f"- [{article['ì œëª©']}]({article['ë§í¬']})  \n"
                        f"  â± {article['ë‚ ì§œ']} | ðŸ“° {article['ì–¸ë¡ ì‚¬']}"
                    )

    with col2:
        st.markdown("### ðŸ“š ì£¼ì œë³„ ì¶”ì²œ ë‰´ìŠ¤")
        extra_topics = ["êµê³¼ì„œ", "AI ë””ì§€í„¸êµê³¼ì„œ", "ë¹„ìƒêµìœ¡", "ì²œìž¬êµìœ¡", "ì²œìž¬êµê³¼ì„œ", "ë¯¸ëž˜ì—”", "ì•„ì´ìŠ¤í¬ë¦¼ë¯¸ë””ì–´", "ë™ì•„ì¶œíŒ", "ì§€í•™ì‚¬"]
        for topic in extra_topics:
            with st.expander(f"ðŸ“˜ {topic} ê´€ë ¨ ë‰´ìŠ¤"):
                articles = crawl_news(topic)
                for article in articles:
                    st.markdown(
                        f"- [{article['ì œëª©']}]({article['ë§í¬']})  \n"
                        f"  â± {article['ë‚ ì§œ']} | ðŸ“° {article['ì–¸ë¡ ì‚¬']}"
                    )
