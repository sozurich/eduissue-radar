# EduIssue Radar - Streamlit ì•± (í”„ë¡œí† íƒ€ì…)

import streamlit as st
import pandas as pd
import re
from collections import Counter
from datetime import datetime
import requests
from bs4 import BeautifulSoup

# 1. í…ìŠ¤íŠ¸ íŒŒì¼ íŒŒì‹± í•¨ìˆ˜ (ë‚ ì§œë³„ë¡œ ë¶„ë¦¬)
def parse_kakao_text(file):
    text = file.read().decode('utf-8')
    date_pattern = r'-{10,}\s*(\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼.*?)\s*-{10,}'
    date_headers = re.findall(date_pattern, text)
    messages = re.findall(r'\[(.*?)\] \[(ì˜¤ì „|ì˜¤í›„) (\d{1,2}:\d{2})\] (.+)', text)
    parsed = []
    date_index = 0
    for user, ampm, time, msg in messages:
        hour, minute = map(int, time.split(':'))
        if ampm == 'ì˜¤í›„' and hour != 12:
            hour += 12
        timestamp = f"{hour:02}:{minute:02}"
        date_str = date_headers[min(date_index, len(date_headers)-1)]
        parsed.append({"ë‚ ì§œ": date_str, "ì‚¬ìš©ì": user, "ì‹œê°„": timestamp, "ë©”ì‹œì§€": msg})
        if '---------------' in msg:
            date_index += 1
    return pd.DataFrame(parsed)

# 2. í‚¤ì›Œë“œ ê¸°ë°˜ ë¯¼ì› ë©”ì‹œì§€ í•„í„°ë§
issue_keywords = ["ë°°ì†¡", "ì§€ì—°", "ëˆ„ë½", "ë¶ˆëŸ‰", "ë¶€ì¡±", "ì •ì‚°", "ë°˜í’ˆ", "ì¶”ê°€", "ì˜¤ë¥˜"]

def extract_issues(df):
    issue_msgs = df[df['ë©”ì‹œì§€'].str.contains('|'.join(issue_keywords))]
    all_words = ' '.join(issue_msgs['ë©”ì‹œì§€'].tolist())
    nouns = re.findall(r'[\uAC00-\uD7A3]+', all_words)
    count = Counter(nouns)
    return issue_msgs, count.most_common(10)

# 3. ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í•¨ìˆ˜ (ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰)
def crawl_news(query):
    headers = {"User-Agent": "Mozilla/5.0"}
    url = f"https://search.naver.com/search.naver?where=news&query={query}"
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, 'html.parser')
    news_items = soup.select(".list_news .news_area")
    results = []
    for item in news_items[:5]:
        title_tag = item.select_one(".news_tit")
        if title_tag:
            title = title_tag.text
            link = title_tag['href']
            press = item.select_one(".info_group span").text if item.select_one(".info_group span") else "ì–¸ë¡ ì‚¬ ë¯¸í™•ì¸"
            results.append({"ì œëª©": title, "ë§í¬": link, "ì–¸ë¡ ì‚¬": press})
    return results

# 4. Streamlit ì¸í„°í˜ì´ìŠ¤
st.title("ğŸ“š EduIssue Radar")
st.markdown("êµê³¼ì„œ ë¯¼ì› ë©”ì‹œì§€ + ë‰´ìŠ¤ í‚¤ì›Œë“œ í†µí•© ë¶„ì„ê¸°")

uploaded_file = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ ì±„íŒ… .txt íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="txt")

if uploaded_file:
    df = parse_kakao_text(uploaded_file)
    df['ë‚ ì§œ'] = df['ë‚ ì§œ'].fillna(method='ffill')
    date_options = sorted(df['ë‚ ì§œ'].unique())
    selected_date = st.selectbox("ë¶„ì„í•  ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”", date_options)
    df_selected = df[df['ë‚ ì§œ'] == selected_date]

    st.success(f"{selected_date} ë‚ ì§œì˜ ë©”ì‹œì§€ {len(df_selected)}ê±´ ë¶„ì„ ì¤‘...")

    issue_df, top_keywords = extract_issues(df_selected)
    st.subheader("ğŸ” ë¯¼ì› ë©”ì‹œì§€ ìš”ì•½")
    st.write(issue_df[['ì‹œê°„', 'ì‚¬ìš©ì', 'ë©”ì‹œì§€']])

    st.subheader("ğŸ”¥ ìì£¼ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ")
    for word, freq in top_keywords:
        st.write(f"- {word} ({freq}íšŒ)")

    st.subheader("ğŸ“° ì—°ê´€ ë‰´ìŠ¤ ê¸°ì‚¬")
    for word, _ in top_keywords[:3]:
        st.markdown(f"**ğŸ” {word} ê´€ë ¨ ë‰´ìŠ¤**")
        articles = crawl_news(word + " êµê³¼ì„œ")
        for article in articles:
            st.markdown(f"- [{article['ì œëª©']}]({article['ë§í¬']}) <{article['ì–¸ë¡ ì‚¬']}>")

    # ğŸ¯ ì¶”ê°€ ì£¼ì œë³„ ë‰´ìŠ¤ ê¸°ì‚¬
    st.subheader("ğŸ“Œ ì£¼ì œë³„ ì¶”ì²œ ë‰´ìŠ¤")
    extra_topics = ["êµê³¼ì„œ", "AI ë””ì§€í„¸êµê³¼ì„œ", "ë¹„ìƒêµìœ¡", "ì²œì¬êµìœ¡", "ì²œì¬êµê³¼ì„œ", "ë¯¸ë˜ì—”", "ì•„ì´ìŠ¤í¬ë¦¼ë¯¸ë””ì–´", "ë™ì•„ì¶œíŒ", "ì§€í•™ì‚¬"]
    for topic in extra_topics:
        st.markdown(f"**ğŸ“š {topic} ê´€ë ¨ ë‰´ìŠ¤**")
        articles = crawl_news(topic)
        for article in articles:
            st.markdown(f"- [{article['ì œëª©']}]({article['ë§í¬']}) <{article['ì–¸ë¡ ì‚¬']}>")
